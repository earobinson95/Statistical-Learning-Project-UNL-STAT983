---
title: 
    title: Classification and Resampling of Wine Quality on Imbalanced Data
    subtitle: Statistical Learning (STAT 983)
author:
  - name: Alison Kleffner
    affil: a
    email: akleffner@huskers.unl.edu
  - name: Sarah Aurit
    affil: a
    email: SarahAurit@creighton.edu
  - name: Emily Robinson
    affil: a
    email: emily.robinson@huskers.unl.edu
affiliation:
  - id: a
    institution: Department of Statistics, University of Nebraska - Lincoln
abstract: '**Purpose:** The ability to assess the quality of wine is essential in the wine industry in order to market and sell a wine. Currently quality assessment is done by the use of human experts, which is costly and time consuming. We examined, through the use of data mining techniques, wine quality assessment through the use of readily available physiochemical wine properties. **Methods:** Tree-based ensemble methods were tuned to classify quality, which was designated as low, normal, and high. We employed the use of eXtreme Gradient Boosting and Random Forest tree-based ensemble methods. Additionally, due to the imbalanced nature of the quality class for which the majority of the data were classified as normal, undersampling and oversampling resampling techniques were used to evaluate model performance. **Results:** Utilizing cross-validation techniques, we saw immense benefits of oversampling the training data set. We consistently saw excellent performance and reached 100\% classification rate of all quality classes with both classifiers. **Conclusions:** These results revealed that oversampling outperformed undersampling and that our two classifiers performed with similar accuracy with XGBoost having a slight advantage in classification of low and high quality when no resampling or undersampling was performed.'
bibliography: references.bib
output: 
  pdf_document:
    template: template.tex
    include:
      after_body: appendix.tex
preamble:
  \usepackage[dvipsnames]{xcolor} % colors
  \newcommand{\er}[1]{{\textcolor{blue}{#1}}}
  \newcommand{\ak}[1]{{\textcolor{RedViolet}{#1}}}
  \newcommand{\sa}[1]{{\textcolor{OliveGreen}{#1}}}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T,
                      warning = F,
                      message = F,
	                    fig.align = "center")
```

```{r load-libraries, include = FALSE}
library(readr)
library(tidyverse)
library(knitr)
library(ggcorrplot)
library(ggforce)
library(reshape2)
library(Hmisc)
library(Gmisc)
```

<!-- \er{Emily} -->
<!-- \ak{Alison} -->
<!-- \sa{Sarah} -->

<!-- Note: Putting each sentence on it's own line helps GitHub with merge conflicts. This does not affect the readability and keeps paragraphs together once knitted -->

# Introduction 

Successful marketing campaigns and productive selling strategies are directly linked to communication about key indicators of quality; hence, objective measurements of quality are essential. 
Within the wine industry, there are two types of quality assessment: physiochemical and sensory tests. 
Sensory tests require a human expert to assess the quality of wine based on visual, taste, and smell \citep{hu2016classification}. 
Hiring human experts to conduct sensory tests can take time and be expensive \citep{gupta2018selection}. 
In addition, taste is the least understood of all human senses, thus there is the potential for human bias to come into play with human testers \citep{cortez2009using}. 
Unlike sensory tests, laboratory tests for measuring the physiochemical characteristics of wine such as acidity and alcohol content do not require a human expert. 
The relationship between physiochemcial and sensory analysis is not well understood. 
Recently, research in the food industry has utilized statistical learning techniques to evaluate widely available characteristics of wine. 
This type of evaluation allows the automation of quality assessment processes by minimizing the need of human experts \citep{gupta2018selection}. 
Additionally, if human experts are used, the classification model can be used to speed up and improve their quality assessment \citep{cortez2009using}.  
These techniques also have the advantage of identifying the importantance of the phsiochemical characteristics that have an impact on the quality of wine as determined by a sensory test.

There have been a few previous papers that have looked into the classification of wine. \cite{cortez2009using} and \cite{gupta2018selection} examined the data using regression methods with their response being a quality rating from 0 to 10, with 0 being the worst quality and 10 being the highest quality. The methods that they employed were multiple regression, neural networks, and support vector machines, and they both found that support vector machines produced the lowest misclassification percentage. In Hu et al [2016], a different approach to this problem was employed where instead of using quality categories from 0-10, they grouped the qualities into three different categories: low, normal, and high. They looked at only data relating to white wine. The classification methods that they employed were Adaptive Boosting and Random Forest. Additionally, due to most of the qualities being classified as “normal”, they employed an oversampling technique, SMOTE in order to develop better classification on the categories with not as much data, in this case “low” and “high”. They found that Random Forest had the lowest misclassification rate of the two at 5.4% \citep{hu2016classification}. 

The goal of this paper, following \cite{hu2016classification} is to train a model that would work well to classify wines into three categories, which are: low quality, normal quality and high quality. 
We evaluated two classification techniques, eXtreme Gradient Boosting (XGBoost) and Random Forest.
Given prior investigation of the white wine data showed the Random Forests technique performed well; we would like to test this method using both white and red wine observations. 
Additionally,  previous papers on the white wine data set have also applied different versions of gradient boosting such as adaptive boost; therefore we will apply XGBoost, an alternative gradient boosting technique. 
The quality categorization poses a challenge of working with imbalanced classes as there are many more normal quality wines than low or high quality wines [Figure \ref{fig:class-distribution}]. 
To address this, we will evaluate different resampling techniques in order to determine if resampling improves performance and to identify which resampling method is best. 
Accuracy of each classification model applied in conjunction with each resampling method was compared. Initially, we first evaluated the classifier performance with no resampling. 
Since the quality class is highly imbalanced, we hypothesized no resampling to have low performance but provide a baseline. We will then apply a resampling method using SMOTE, which is an algorithm where the minority class (in this case low and high quality), will be oversampled, as this technique seemed to perform well in previous research \citep{hu2016classification}. 
The final resampling method we applied is a random undersampling method, where majority class data are randomly removed from the data set. 
We hypothesize random undersampling risks losing valuable information in our model. 
Accuracy of each classification method and resampling technique was evaluated by the overall correct classification rate and each individual group (low quality, normal quality, and high quality) correct classification rate.

# Data Exploration

## About the Data

In this paper, we applied classification and resampling techniques to the “Wine Quality Data Set” found on the UCI Data Repository \citep{UCIdataset}. 
The data consist of information from samples of Vinho Verde, which is a product from the northwest region of Portugal. 
The data was collected from May, 2004 to February, 2007, and is made up of 1599 red wines and 4898 white wines for a total of 6493 observations \citep{cortez2009using}. 
For each of the wines in the dataset, 11 physiochemical measurements were taken on it: fxed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol. 
Additionally, we classified the combined red and white wine data sets, and included an additional predictor variable categorizing the given observation as red or white wine. 
The response was measured as a quality rating based on a sensory test carried out by at least three sommeliers, where a 0 was considered very bad and a 10 was excellent \cite{gupta2018selection}. 
Following \cite{hu2016classification}, we separated the wine into three quality classes: Low Quality $(\le 4)$, Normal (5-7), and High quality $(\ge 8)$. 
These response values are imbalanced as can be seen in Figure \ref{fig:class-distribution}, with most of the wines having a response of “Normal”. 

```{r data}
winequality <- read.csv("../data/winequality-all.csv")
```

```{r class-distribution, fig.height = 2.5, fig.width = 5, fig.align = 'center', fig.cap = "Wine quality class imbalance."}
winequality %>%
  mutate(qualityclass = factor(qualityclass, levels = c("Low", "Normal", "High")),
         type = factor(type, levels = c("red", "white"), labels = c("Red Wine", "White Wine"))) %>%
  group_by(type, qualityclass) %>%
  summarise(count = n()) %>%
  mutate(prop = count/sum(count)) %>%
  ungroup() %>%
  ggplot(aes(x = qualityclass, y = prop, fill = qualityclass, label = paste(100*round(prop, 3), "%", sep = ""))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(vjust = -0.5, size = 3) +
  facet_grid(~type) +
  theme_bw() +
  theme(aspect.ratio = 0.6) +
  scale_fill_brewer(palette = "Paired")+
  scale_x_discrete("Quality Class") +
  scale_y_continuous("Proportion", limits = c(0, 1.1), breaks = seq(0, 1, 0.2), labels = scales::percent)
```

## Exploring the Data

In Figure \ref{fig:boxplots}, boxplots of the 11 physiochemical variables are given. 
As can be see in this plot, for most of the predictor variables there are points that would be considered outliers, which are represented by black dots. 
We did not consider removing outliers, so no data points were removed for our final analysis. 
Looking at the boxplots, one can use these to give an idea of which of these predictor variables may be helpful in helping to determine the classification of the wines. 
For example, for alcohol, the interquartile range of the High category is above that of the Normal and Low class. 

```{r boxplots, fig.height = 5, fig.cap="Distribution of the predictor variables."}
winequality_noq = winequality[,-1]
data_long <- melt(winequality_noq)
data_long %>%
  mutate(qualityclass = factor(qualityclass, levels = c("Low", "Normal", "High"))) %>%
ggplot(aes(x = qualityclass, y = value)) +            
  geom_boxplot() +
  facet_wrap(~variable, scales = "free") +
  theme_bw()
```


Additionally, Table \ref{tab:descriptives-table} provides physiochemical characteristics stratified by three quality classes. Continuous variables are presented as median and interquartile range (IQR) whereas discrete variables are presented as counts and proportions (%). 
Data were stratified by three classes of quality and comparisons were made with the Kruskal-Wallis and chi-square tests for continuous and discrete variables, respectively. 
We did not conduct post-hoc testing to further investigate pairwise differences. 
There was statistical evidence of a difference between the three classes of quality for all variables except for pH (P < 0.001, respectively)
The low designation of wine quality was associated with the highest amount of red wine, chlorides, density, fixed acidity, and volatile acidity, and the lowest amount of alcohol, citric acid, free sulfur dioxide, residual sugar, sulphates, and total sulfur dioxide.

```{r descriptives}
# https://cran.r-project.org/web/packages/Gmisc/vignettes/Descriptives.html;

#Style all of the table output using htmlTable's theme handler
htmlTable::setHtmlTableTheme(css.rgroup = "")

winequality$qualityclass <- factor(winequality$qualityclass, levels=c("Low","Normal","High"),
                            #labels=c("Low (N=246)","Normal (N=6,053)","High (N=198)")
                            )
winequality$type <- factor(winequality$type, levels=c("red","white"),
                    labels=c("Red Wine","White Wine"))

label(winequality$qualityclass)         <- "Quality"
label(winequality$type)                 <- "Type"
label(winequality$fixed.acidity)        <- "Fixed Acidity"
units(winequality$fixed.acidity)        <- "(g tartaric acid/dm^3), median (IQR)"
label(winequality$volatile.acidity)     <- "Volatile Acidity"
units(winequality$volatile.acidity)     <- "(g acetic acid/dm^3), median (IQR)"
label(winequality$citric.acid)          <- "Citric Acid"
units(winequality$citric.acid)          <- "(g/dm^3), median (IQR)"
label(winequality$residual.sugar)       <- "Residual Sugar"
units(winequality$residual.sugar)       <- "(g/dm^3), median (IQR)"
label(winequality$chlorides)            <- "Chlorides"
units(winequality$chlorides)            <- "(g sodium chloride/dm^3), median (IQR)"
label(winequality$free.sulfur.dioxide)  <- "Free Sulfur Dioxide"
units(winequality$free.sulfur.dioxide)  <- "(mg/dm^3), median (IQR)"
label(winequality$total.sulfur.dioxide) <- "Total Sulfur Dioxide"
units(winequality$total.sulfur.dioxide) <- "(mg/dm^3), median (IQR)"
label(winequality$density)              <- "Density"
units(winequality$density)              <- "(g/cm^3), median (IQR)"
label(winequality$pH)                   <- "pH"
units(winequality$pH)                   <- "median (IQR)"
label(winequality$sulphates)            <- "Sulphates"
units(winequality$sulphates)            <- "(g potassium sulphate/dm^3), median (IQR)"
label(winequality$alcohol)              <- "Alcohol"
units(winequality$alcohol)              <- "(volume %), median (IQR)"

getTable1Stats <- function(x, digits = 2, ...){
  getDescriptionStatsBy(x = x,
                        by = winequality$qualityclass,
                        digits = digits,
                        continuous_fn = describeMedian,
                        header_count = TRUE,
                        NEJMstyle = TRUE,
                        html = FALSE)
}

t1 <- list()
# t1[["Red vs. White Wine Type"]] <- getTable1Stats(winequality$type)
t1[["Fixed Acidity"]] <- getTable1Stats(winequality$fixed.acidity)
t1[["Volatile Acidity"]] <- getTable1Stats(winequality$volatile.acidity)
t1[["CitricAcidity"]] <- getTable1Stats(winequality$citric.acid)
t1[["Residual Sugar"]] <- getTable1Stats(winequality$residual.sugar)
t1[["Chlorides"]] <- getTable1Stats(winequality$chlorides)
t1[["Free Sulfur Dioxide"]] <- getTable1Stats(winequality$free.sulfur.dioxide)
t1[["Total Sulfur Dioxide"]] <- getTable1Stats(winequality$total.sulfur.dioxide)
t1[["Density"]] <- getTable1Stats(winequality$density)
t1[["pH"]] <- getTable1Stats(winequality$pH)
t1[["Sulphates"]] <- getTable1Stats(winequality$sulphates)
t1[["Alcohol"]] <- getTable1Stats(winequality$alcohol)

P <- c('< 0.001',	'< 0.001',	'< 0.001',	'< 0.001',	'< 0.001',	'< 0.001',	'0.3496',	'< 0.001',	'< 0.001',	'0.0025',	'< 0.001')

library(magrittr)
mergeDesc(t1) %>% cbind(P) %>% knitr::kable("latex", digits = 2, escape = F, booktabs = T, linesep = "", align = "c", label = "descriptives-table", caption = "Basic Descriptive Statistics from the Wine Quality Dataset.")
```

Figure \ref{fig:correlation} gives the correlation plot for the 11 physiochemical variables, with also the 0-10 scale for the quality of the wines. 
Correlations that are not considered to be statistically significant are not shown. 
Most of the correlations seem to be on the lower end of the spectrum, so multicollinearity does not seem to be a huge issue that needs to be addressed among the predictor variables. 
It also shows that all of the predictor variables have a statistically significant correlation with the response of quality, so they should be helpful for classification. 

```{r correlation, fig.height = 4, fig.cap= "Correlation Plot"}
corr <- round(cor(winequality[c(1, 4:14)]), 3)
p.mat <- cor_pmat(winequality[c(1, 4:14)])
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           outline.col = "white",
           colors = c("#6D9EC1", "white", "#E46726"),
           p.mat = p.mat,
           lab = TRUE,
           insig = "blank",
           lab_size = 2)

```

# Statistical Methods

We applied two classification methods, Random Forest and eXtreme Gradient Boosting (XGBoost), on the original training set (no resampling), the undersampled training set, and the oversampled training set. 
Accuracy of each classification method and resampling technique was evaluated by the overall correct classification rate and each individual group (low quality, normal quality, and high quality) correct classification rate. Classification rate is calculated as:
\begin{align*}
\text{overall accuracy} &= \frac{1}{n}\sum_{i=1}^{n} I_{[\widehat{class_i}=class_i]} \\
\text{accuracy for group k} &= \frac{1}{n_k}\sum_{j=1}^{n_k} I_{[\widehat{class_j}=class_j]}. \\
\end{align*}
Monte Carlo Cross-Validation (MCMC) was used to select tuning parameters and evaluate model performance.
The MCMC algorithm repeatedly splits the data into training and testing sets (B = 50 for model tuning; B = 100 for final model comparison) by randomly selecting the designated proportions (70% training; 30% testing) of the overall data set to each. 
The undersampling and oversampling techniques as described below are then applied to the training set to obtain a final resampled training data set. 
For each split, the final resampled training data set is used to build the model; then accuracy is evaluated on the corresponding testing data set. 
The mean, 5th-quantile, and 95th-quantile of the correct classification rates are used to evaluate performance over the B splits. 
Both classification methods were tuned by conducting a hyperparameter grid search with MCMC (B = 50) to minimize the overall accuracy [Appendix \ref{app:tuning}]; parallel computing with the `furrr` package in R was conducted to minimize computing time \citep{furrr}.

## Resampling Techniques

Before we are able to consider the different classification methods, we must first examine the imbalanced issue in our data (Figure \ref{fig:class-distribution}) and develop our resampling techniques. 
This is because typical classifier algorithms assume a relatively balanced distribution across the classes, so with imbalanced data there tends to be bias towards the majority class \citep{sun2009classification}. 
Classification rules for the minority classes tend to be undiscovered or ignored, so the minority class is misclassified more often than the majority class. 
If we were to look at our data set from a binary standpoint where “Low Quality” and “High Quality” were classified as "rare", and “Normal Quality” was classified as "not rare", the majority (not rare) class has 6053 observations, and the minority (rare) class has 444 observations, which is a ratio of about 13:1. 
Due to this imbalanced nature in the data, classifying algorithms will be biased towards classifying wines as "Normal", causing poor predictions for the "Low" and "High" quality classes. 
When creating the resampled data sets, the binary classification of “rare” and “not rare” is used due to the resampling algorithms relying on a binary class designation.

A method on how to lower the effects on working with an imbalanced data set is through resampling the original data set either by either oversampling the majority class, or undersampling the minority class. 
The goal with these methods is to create a data set that has close to a balanced class distribution, so the classifying algorithms will have better predictions due to being able to predict to the minority class more accurately \citep{hoens2013imbalanced}. 
To see how valuable resampling was in our data set, we ran the classifying algorithms with the original imbalanced data set, a random undersampling method, and an a Synthetic Minority Over-Sampling Technique (SMOTE), which is an oversampling method.
We initially evaluated the classifier performance with the original imbalanced data set (without resampling), and hypothesized that it would be associated with low performance, however, we felt it provided an appropriate baseline. 

The undersampling method that we considered is random undersampling. 
In this method, instances of the majority class are discarded at random until reaches balanced with the minority class \citep{hoens2013imbalanced}. 
For example, say there are 1000 observations in the majority class, and 100 in the minority class, observations in the majority class will be randomly discarded until this class is also 100. 
The benefit of this method is that since the data frame is being reduced, it is less costly. 
On the other hand, potentially useful information is discarded, which may make the decision boundary between the majority and minority class less clear. 
We hypothesize this discarding of information may cause a decrease in overall prediction performance \citep{hoens2013imbalanced}. 

Next, we handled imbalanced data through resampling the original data set by oversampling the majority class using the SMOTE method, which is a technique that uses interpolation of the minority class to create synthetic data. 
The process begins by finding the k-nearest neighbors of each observation of the minority class based on some distance measure. 
Then a point between the minority class observation and one of its nearest neighbors is randomly picked by first finding the difference between the observation and its nearest neighbor. 
This difference is then multiplied by a random number between 0 and 1. 
That number is then added to the observation, which becomes the new synthetic data point that is then added to the data set \citep{chawla2002smote}.
Through the use of a hperparameter grid search with MCMC, the number of neighbors chosen was k = 3 [Appendix \ref{app:xgb-tuning}].
Oversampling tends to have an overfitting problem since now the minority class extends into the majority space, however this generally poses less of an issue with SMOTE \citep{luengo2011addressing}. 
This was ran using the SMOTE function in the `smotefamily` package in R \citep{smotefamily}.  

## Classification Methods

We evaluated two classifiers, Random Forest and eXtreme Boosting (XGBoost), both tree based ensemble methods.
First, we utilized the Random Forest algorithm and the associated selection of independent variables for the Random Forest with the `randomForest` library in R \citep{randomForest}. 
In general, a Random Forest algorithm involves the generation of many classification trees, which are each grown through recursive partitioning of independent variable space. 
During the growth phase for each classification tree, split points of variable space are investigated to maximize the reduction of heterogeneity, which is measured with the Gini impurity index (a reflection of the probability of a variable being wrongly chosen through a stochastic process). 
Each tree is then grown to a phase where node(s) represent a delineation of data per split point of each variable, and leaves are the terminal points that represent the final classification of data. 
After the growth phase, a leaf is potentially pruned back to a node to maximize the trade-off between complexity and rate of misclassification. 
A Random Forest incorporates many trees and subsequently aggregates the predictions made by each tree. 
We selected the number of trees and number of variables to be randomly sampled from to generate a split point at each node (mtry) by conducting a hyperparameter grid search with MCMC [Appendix \ref{app:rf-tuning}].

initially selected 500 trees and four variables that could be randomly sampled from to generate a split point at each node. 
Finally, a stepwise function was created to iteratively conduct the Random Forest algorithm for the purposes of selection of the optimal number of variables to sample from.

We also applied eXtreme Gradient Boosting (XGBoost) using the `xgboost` function with the multiclass classification using the softmax objective function with three classes maximizing the multiclass misclassfication error evaluation metric in the xgboost library \citep{xgboost}. 
This method combines the tree-based model approach by implementing recursive partitioning and the boosting algorithm which repeatedly optimizes classification methods on the training set. 
In repeated optimization, a weak classifier is fit on the original data set with each observation having equal weight, the weight is then calculated for the current model based on the error rate and observations are assigned new weights used to fit the next weak classifier. 
This process is repeated for a final boosted classifier given by the weighted sum of our weak classifiers. 
XGBoost allows for a variety of evaluation metrics providing a benefit over other boosting methods. 
Tuning parameters for maximum tree depth, step size shrinkage to prevent overfitting (eta), maximum number of boosting iterations, and number of threads used for parallel computing were selected by conducting a hyperparameter grid search with MCMC [Appendix \ref{app:xgb-tuning}].

# Results 

A comparison of the best classifiers is shown in Figure \ref{fig:accuracy-results}. 
With no resampling, our overall baseline classification rate for Random Forest was between 93.7\% and 94.9\% while the rare classes had baseline classification rates of 9\% and 30.9\% for Low and High Quality respectively. 
The XGBoost had a similar overall baseline classification rate between 93.2\% and 94.9\% with a slight improvement in accuracy of Low and High Quality classes over Random Forest with 14.5\% and 33\% baseline classification rates. 
When undersampling on the training data set is conducted, Random Forest performs similar to the baseline in terms of overall accuracy with a classification rate between 93.4\% to 94.9\%, but performs better in terms of Low Quality and High Quality classification rates at 15\% and 33\% respectively. 
However, with undersampling, XGBoost results in a slight decrease from the baseline with an overall classification rate between 92.3\% and 94\%. 
We see this decrease occur from a sacrifice in classification of Normal Quality wines at 97.8\%, but an increase in the Low Quality, 24.1\%, and High Quality, 34.8\% classification rates. 
We saw immense benefits of oversampling the training data set reaching consistent performance of 100\% classification rate of all quality classes with both classifiers over all MCMC splits. 
These results indicate that oversampling outperformed undersampling and that accuracy is similar between classifiers with a slight advantage in the XGBoost classifier over Random Forest for the rare quality classification when no resampling or random undersampling is applied. 

```{r accuracy-results, fig.height = 6, fig.cap = "MCMC accuracy results for each classifier applied to each resampling technique."}

# read in data
MCMC.results <- read_csv("MCMC.results.csv")

# new facet labels
accuracyGroup.labs <- c("Overall", "Low Quality", "Normal Quality", "High Quality")
names(accuracyGroup.labs) <- c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high")

# plot accuracy
MCMC.results.plot <- MCMC.results %>%
  mutate(samplingMethod = factor(samplingMethod, levels = c("none", "undersample", "oversample"), labels = c("No Resampling", "Undersampling", "Oversampling")),
         accuracyGroup = factor(accuracyGroup, levels = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"))) %>%
  ggplot(aes(x = mean, y = samplingMethod, group = Method, color = Method)) +
  geom_point(size = 1, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(xmin = lower, xmax = upper), position = position_dodge(width = 0.5), width = 0.4) +
  # facet_zoom(xlim = c(0.90, 1)) +
  facet_wrap(~accuracyGroup, ncol = 1, labeller = labeller(accuracyGroup = accuracyGroup.labs)) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_y_discrete("") +
  scale_x_continuous("Correct Classification Rate", limits = c(0, 1), breaks = seq(0,1,0.2), labels = scales::percent) +
  scale_color_brewer("", palette = "Paired")
MCMC.results.plot
```

# Discussion and Conclusion

As previously noted, due to the imbalanced nature of the quality class, classifying algorithms were biased towards categorizing wines as Normal quality, and causing poor predictions for the Low and High quality classes. 
By employing resampling methods, we increased the classification of the minority classes. 
While random undersampling showed improvement in the classification rates of the low and high classes, oversampling using the SMOTE algorithm outperformed both the undersampling method and no resampling. 
After tuning our classifiers, we reached a consistent perfect quality classification for both red and white wines. 
Both classifiers, Random Forest and XGBoost, performed similar in overall performance. 
We see a slight advantage of XGBoost in the classification of the rare classes when undersampling of the training data or no resampling of the training data is applied. 
This advantage arises from the boosting technique of reassigning higher weights to observations misclassified at each boosting iteration. 
While XGBoost indicated a slight advantage, tuning the classifier was more intensive than the tuning required to implement the Random Forest classifier. 
Our results support previous research of strong performance from ensemble based methods. 
We saw our best classification rates when applying the SMOTE oversampling algorithm to the training data set. Future research in alternate undersampling methods is necessary to provide a comparison against the random undersampling method used in this paper.
Additional model validation should be conducted on wine from other vineyards and regions in order to implement the use of a tuned model across the wine industry for quality classification.

# Supplementary Material {-}

+ **Data:** Data used was from the [UCI Data Repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) \citep{UCIdataset}.
+ **Code:** Access the final analysis code on [GitHub](https://github.com/earobinson95/Statistical-Learning-Project-UNL-STAT983/blob/main/reports/final-classification.R) [Appendix \ref{app:code}].

# Course Reflection {-}

We have learned multiple lessons from this course and this project! 
From the paper specifically, we made a point of using this opportunity to learn some new technical skills on top of methods. 
For example, we were able to utilize some collaboration tools, like GitHub, to make working on the project as a group more seamless. 
Additionally, through this project and the homework assignments, we were able to improve our R coding skills, especially with R Markdown. 
Furthermore, the project also reiterated the importance of cross validation and the tuning of the models, as we saw increase in our performance when we spent some time working on the finding the best tuning parameters. 
Having a proposal due weeks before the final project due date was also helpful in pacing us through this project, as time management is an important skill. 
As for the course, we really appreciated how well prepared you were for each lecture and that we received timely feedback (like going through some of the theory homework questions in class). 
We also like how all of the material of the course felt accessible, which made it easy to follow along and learn. This also helped with the desire to learn, as the methods were described in an understandable way. 
We enjoyed the heavy emphasis on the importance of interpretation and communication, as the use of complex methods is of no use if readers are not able to understand it. 
This practice with communication will help in future careers. 
The only suggestion we had was to potentially spend some more time on coding in class, however the homework as very helpful with this aspect. 
Overall we really enjoyed the course and have learned many valuable things from it!  


```{r, include = F}
# devtools::install_github("crsh/papaja") 
library(papaja)
render_appendix(
"appendix.Rmd",
bibliography = rmarkdown::metadata$bibliography,
csl = rmarkdown::metadata$csl,
quiet = TRUE
)
```
