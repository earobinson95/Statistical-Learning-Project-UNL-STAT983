---
title: |
  Final Project Report: Statistical Learning (UNL STAT 983)
author:
  - name: Alison Kleffner
    affil: a
  - name: Sarah Aurit
    affil: a
  - name: Emily Robinson
    affil: a
affiliation:
  - num: a
    address: |
      Department of Statistics, University of Nebraska - Lincoln,
bibliography: references.bib
output: 
  pdf_document:
    template: template.tex
    include:
      after_body: appendix.tex
preamble:
  \usepackage[dvipsnames]{xcolor} % colors
  \newcommand{\er}[1]{{\textcolor{blue}{#1}}}
  \newcommand{\ak}[1]{{\textcolor{RedViolet}{#1}}}
  \newcommand{\sa}[1]{{\textcolor{OliveGreen}{#1}}}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, 
                      eval = T,
                      warning = F,
                      message = F,
	                    fig.align = "center")
```

```{r load-libraries, include = FALSE}
library(readr)
library(tidyverse)
library(knitr)
library(ggcorrplot)
library(reshape2)
library(Hmisc)
library(Gmisc)
library(randomForest)
library(readr)
library(tictoc) 
library(furrr)
library(smotefamily)
```

<!-- \er{Emily} -->
<!-- \ak{Alison} -->
<!-- \sa{Sarah} -->

###  Introduction  ###

Successful marketing campaigns and productive selling strategies are directly linked to communication about key indicators of quality; hence, objective measurements of quality are essential. 
Within the wine industry, there are two types of quality assessment: physiochemical and sensory tests. 
Sensory tests require a human expert to assess the quality of wine based on visual, taste, and smell \citep{hu2016classification}. 
Hiring human experts to conduct sensory tests can take time and be expensive \citep{gupta2018selection}. 
In addition, taste is the least understood of all human senses \citep{cortez2009using}. 
Unlike sensory tests, laboratory tests for measuring the physiochemical characteristics of wine such as acidity and alcohol content do not require a human expert. 
The relationship between physiochemcial and sensory analysis is not well understood. 
Recently, research in the food industry has utilized statistical learning techniques to evaluate widely available characteristics of wine. It is desirable to classify wines using physicochemical properties since this does not involve human bias that would come into play with human tasters. This type of evaluation allows the automation of quality assessment processes by minimizing the need of human experts \citep{gupta2018selection}. 
These techniques also have the advantage of identifying important the physiochemical characteristics that have an impact on the quality of wine as determined by a sensory test.

The goal of this paper was to generate models that would utilize the objective physiochemical characteristics well to classify wines into three categories: poor quality, normal quality and high quality. 

###  Methods  ###

# Data Source; Variables and Outcomes of Interest

We applied classication and resampling techniques to the “Wine Quality Data Set” found on the UCI Data Repository at https://archive.ics.uci.edu/ml/datasets/Wine+Quality. [UCI, Cortez et al., 2009]. The data consist of information from samples of vinho verde, which is a product from the northwest region of Portugal. The data were collected from May, 2004 to February, 2007, and consisted of 1599 red wines and 4898 white wines for a total of 6493 observations [Cortez, 2009]. For each of the wines in the dataset, 11 physiochemical variables were recorded: fxed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol. Additionally, we classified the combined red and white wine data sets, and included an additional predictor variable categorizing the given observation as red or white wine. Our response was a quality rating based on a sensory test carried out by at least three sommeliers, where 0 was considered very bad and 10 was excellent [Gupta, 2018]. Following Hu et al. [2016], we separated the wine into three classes: Low Quality $(\le 4)$, Normal (5-7), and High quality $(\ge 8)$. 

# Statistical Methods

Continuous variables are presented as median and interquartile range (IQR) whereas discrete variables are presented as counts and proportions (%). Data were stratified by three classes of quality and comparisons were made with the Kruskal-Wallis and chi-square tests for continuous and discrete variables, respectively. We did not conduct post-hoc testing to further investigate pairwise differences.

# Statistical Methods: Resampling

The categorization of quality was imbalanced as there were many more normal quality wines than low or high quality wines (Figure \ref{fig:class-distribution}), a significant challenge. Typically, classifier algorithms assume a relatively balanced distribution; therefore, imbalanced data tend to be biased towards the majority class [Yanminsun, 2011]. Classification rules for the minority classes tend to be undiscovered or ignored, so the minority class is misclassified more often than the majority class. To address this, we evaluated different resampling techniques in order to determine if resampling improved performance and to identify which resampling method was best. The goal with these methods is to create a data set that has close to a balanced class distribution, so the classifying algorithms will have better predictions, so it will predict to the minority class more accurately [Chawla, 2013]. We examined the data set from a binary standpoint where “Low” and “High” were classified as the minority (rare), and “Normal” was classified as the majority (not rare). When creating the resampled data sets, the binary classification of “rare” and “not rare” was used due to the resampling algorithms needing a binary class. 

To see how valuable resampling was in our data set, we ran the classifying algorithms with the original imbalanced data set, a Synthetic Minority Over-Sampling Technique (SMOTE), which is an oversampling method, and a random undersampling method. We initially evaluated the classifier performance with the original imbalanced data set (without resampling), and hypothesized that it would be associated with low performance, however, we felt it provided an appropriate baseline. 

Next, we handled imbalanced data through resampling the original data set by oversampling the majority class. We applied a resampling method using SMOTE, an algorithm where the minority class (in this case low and high quality) are oversampled. SMOTE, an oversampling technique uses interpolation of the minority class to create synthetic data. The process begins by finding the k nearest neighbors of each observation of the minority class based on some distance measure. Then a point between the minority class observation and one of its nearest neighbors is randomly picked by first finding the difference between the observation and its nearest neighbor. This difference is then multiplied by a random number between 0 and 1. This is then added to the observation, which becomes the new synthetic data point that is then added to the data set [Chawla, 2002].  In Hu et al, they used k=5, and we mirrored that to create our resampled data set [2016]. Oversampling tends to have an overfitting problem since now the minority class extends into the majority space, however, this generally poses less of an issue with SMOTE [Luego, 2010]. This was ran using the SMOTE function in the smotefamily package in R.   

The final resampling method we applied was a random under sampler. In this method, instances of the majority class are discarded at random until reaches balanced with the minority class [Chawla, 2013]. For example, say there are 1000 observations in the majority class, and 100 in the minority class, observations in the majority class will be randomly discarded until this class is also 100. The benefit of this method is that since the data frame is being reduced, it is less costly. In contrast, since data are being removed, potentially valuable information is not considered [Hu et al, 2016]. We hypothesized that we would be impacted by this loss of information in that when information is discarded, there may be a less clear decision boundary between the majority and minority class, causing a decrease in prediction performance [Chawla, 2013].

Accuracy of each classification method and resampling technique was evaluated by the overall correct classification rate and each individual group (low quality, normal quality, and high quality) correct classification rate.  

#  Statistical Methods: Classification

We evaluated two classification techniques, which included Random Forest and eXtreme Boosting (XGBoost) methodology, both tree based ensemble methods. Prior investigation of the white wine data showed the Random Forests technique performed well; we wanted to test this method using both white and red wine. Previous papers on the wine data set have also applied different versions of gradient boosting such as adaptive boost; we utilized an alternative gradient boosting technique XGBoost. 

The first classficiation technique utilized the Random Forest algorithm and the associated selection of independent variables for the Random Forest with the randomForest and tuneRF functions. In general, a Random Forest algorithm involves the creation of many classification trees, which are each grown through recursive partioning of independent variable space. During the growth phase for each classification tree, split points of variable space are investigated to maximize the reduction of heterogeneity, which is measured with the Gini impurity index (a reflection of the probability of a variable being wrongly chosen through a stochastic process). Each tree is then grown to a phase where node(s) represent a delineation of data per split point of each variable, and leaves are the terminal points that represenet the final classification of data. After the growth phase, a leaf is potentially pruned back to a node to maximize the tradeoff between complexity and rate of misclassification. A random forest incorporates many trees and subsequently aggregates the predictions made by each tree. We initially selected 500 trees and four variables that could be randomly sampled from to generate a split point at each node. Finally, a stepwise function was created to iteratively conduct the Random Forest algorithm for the purposes of selection of the optimal number of variables to sample from.

We also applied eXtreme Gradient Boosting (XGBoost) using the xgboost function with the multiclass classification using the softmax objective function with three classes maximizing the multiclass misclassfication error evaluation metric in the xgboost library \citep{xgboost}. This method combines the tree-based model approach by implementing recursive partitioning and the boosting algorithm which repeatedly optimizes classification methods on the training set. In repeated optimization, a weak classifier is fit on the original data set with each observation having equal weight, the weight is then calculated for the current model based on the error rate and observations are assigned new weights used to fit the next weak classifier. This process is repeated for a final boosted classifier given by the weighted sum of our weak classifiers. XGBoost allows for a variety of evaluation metrics providing a benefit over other boosting methods. Tuning parameters for maximum tree depth, step size shrinkage to prevent overfitting (eta) were selected by conducting a hyperparameter grid search with MCMC.

# Statistical Methods: Cross-Validation

Monte Carlo Cross-Validation (MCMC) was used to select tuning parameters and evaluate model performance. The MCMC algorithm repeatedly split the data into training and testing sets (B = 50) by randomly selecting the designated proportions (70% training; 30% testing) of the overall data set for each iteration. The undersampling and oversampling techniques described below were then applied to the training set to obtain a final resampled training data set. For each split, the final resampled training data set was used to build the model and the accuracy was evaluated on the corresponding testing data set. The mean, 5th-quantile, and 95th-quantile of the correct classification rates are presented as a performance metric for the B splits. 

All analyses were completed with R version 3.6.1; P < 0.05 was deemed significant.

###  Results  ###

The response values were imbalanced as seen in Figure \ref{fig:class-distribution}, with most of the wines having a response of “Normal”. 

```{r data}
winequality <- read.csv("../data/winequality-all.csv")
```

```{r class-distribution, fig.height = 2.5, fig.width = 5, fig.align = 'center', fig.cap = "Wine quality class imbalance."}
winequality %>%
  mutate(qualityclass = factor(qualityclass, levels = c("Low", "Normal", "High"))) %>%
  group_by(type, qualityclass) %>%
  summarise(count = n()) %>%
  mutate(prop = count/sum(count)) %>%
  ungroup() %>%
  ggplot(aes(x = qualityclass, y = prop, fill = qualityclass, label = paste(100*round(prop, 3), "%", sep = ""))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(vjust = -0.5, size = 3) +
  facet_grid(~type) +
  theme_bw() +
  theme(aspect.ratio = 0.6) +
  scale_fill_viridis_d(begin = 0.4, end = 0.8) +
  scale_x_discrete("Quality Class") +
  scale_y_continuous("Proportion", limits = c(0, 1.1), breaks = seq(0, 1, 0.2), labels = scales::percent)
```

# Preliminaries; Sarah put in table and explanation in this section? SJA: sounds great

```{r descriptives}
# https://cran.r-project.org/web/packages/Gmisc/vignettes/Descriptives.html;

#Style all of the table output using htmlTable's theme handler
htmlTable::setHtmlTableTheme(css.rgroup = "")

winequality$qualityclass <- factor(winequality$qualityclass, levels=c("Low","Normal","High"),
                            #labels=c("Low (N=246)","Normal (N=6,053)","High (N=198)")
                            )
winequality$type <- factor(winequality$type, levels=c("red","white"),
                    labels=c("Red Wine","White Wine"))

label(winequality$qualityclass)         <- "Quality"
label(winequality$type)                 <- "Type"
label(winequality$fixed.acidity)        <- "Fixed Acidity"
units(winequality$fixed.acidity)        <- "(g tartaric acid/dm^3), median (IQR)"
label(winequality$volatile.acidity)     <- "Volatile Acidity"
units(winequality$volatile.acidity)     <- "(g acetic acid/dm^3), median (IQR)"
label(winequality$citric.acid)          <- "Citric Acid"
units(winequality$citric.acid)          <- "(g/dm^3), median (IQR)"
label(winequality$residual.sugar)       <- "Residual Sugar"
units(winequality$residual.sugar)       <- "(g/dm^3), median (IQR)"
label(winequality$chlorides)            <- "Chlorides"
units(winequality$chlorides)            <- "(g sodium chloride/dm^3), median (IQR)"
label(winequality$free.sulfur.dioxide)  <- "Free Sulfur Dioxide"
units(winequality$free.sulfur.dioxide)  <- "(mg/dm^3), median (IQR)"
label(winequality$total.sulfur.dioxide) <- "Total Sulfur Dioxide"
units(winequality$total.sulfur.dioxide) <- "(mg/dm^3), median (IQR)"
label(winequality$density)              <- "Density"
units(winequality$density)              <- "(g/cm^3), median (IQR)"
label(winequality$pH)                   <- "pH"
units(winequality$pH)                   <- "median (IQR)"
label(winequality$sulphates)            <- "Sulphates"
units(winequality$sulphates)            <- "(g potassium sulphate/dm^3), median (IQR)"
label(winequality$alcohol)              <- "Alcohol"
units(winequality$alcohol)              <- "(volume %), median (IQR)"

getTable1Stats <- function(x, digits = 0, ...){
  getDescriptionStatsBy(x = x,
                        by = winequality$qualityclass,
                        digits = digits,
                        continuous_fn = describeMedian,
                        header_count = TRUE,
                        NEJMstyle = TRUE,
                        ...)
}

t1 <- list()
#t1[["Red vs. White Wine Type"]] <- getTable1Stats(winequality$type)
t1[["Fixed Acidity"]] <- getTable1Stats(winequality$fixed.acidity)
t1[["Volatile Acidity"]] <- getTable1Stats(winequality$volatile.acidity)
t1[["CitricAcidity"]] <- getTable1Stats(winequality$citric.acid)
t1[["Residual Sugar"]] <- getTable1Stats(winequality$residual.sugar)
t1[["Chlorides"]] <- getTable1Stats(winequality$chlorides)
t1[["Free Sulfur Dioxide"]] <- getTable1Stats(winequality$free.sulfur.dioxide)
t1[["Total Sulfur Dioxide"]] <- getTable1Stats(winequality$total.sulfur.dioxide)
t1[["Density"]] <- getTable1Stats(winequality$density)
t1[["pH"]] <- getTable1Stats(winequality$pH)
t1[["Sulphates"]] <- getTable1Stats(winequality$sulphates)
t1[["Alcohol"]] <- getTable1Stats(winequality$alcohol)

library(magrittr)
mergeDesc(t1,getTable1Stats(winequality$type)) %>%
          htmlTable(caption  = "Table 1. Basic Descriptive Statistics from the Wine Quality Dataset",
                    tfoot = "Continuous variables are presented as median (IQR) and discrete variables are presented as count and proportion (%)")
t1

```
Table 1 provides physiochemical characteristics stratified by three quality classes. There was statistical evidence of a difference between the three classes of quality for all variables except for pH (P < 0.001, respectively). The low designation of wine quality was associated with the highest amount of red wine, chlorides, density, fixed acidity, and volatile acidity, and the lowest amount of alcohol, citric acid, free sulfur dioxide, residual sugar, sulphates, and total sulfur dioxide.

In Figure \ref{fig:boxplots}, boxplots of the 11 physiochemical variables are given. As can be see in this plot, for most of the predictor variables there are points that would be considered outliers, which are represented by black dots. We did not consider removing outliers as there was no evidence that any of these points represented erroneous data, so no data points were removed for our final analysis. Looking at the boxplots, one can use these to give an idea of which of these predictor variables may be helpful in helping to determine the classification of the wines. For example, for alcohol, the interquartile range of the High category is above that of the Normal and Low class. Figure \ref{fig:correlation}, gives the correlation plot for the 11 physiochemical variables, with also the 0-10 scale for the quality of the wines. Correlations that are not considered to be statistically significant are not shown. Most of the correlations seem to be on the lower end of the spectrum, so multicollinearity does not seem to be a huge issue that needs to be addressed among the predictor variables.It also shows that all of the predictor variables have a statistically significant correlation with the response of quality, so they should be helpful for classification. 

```{r boxplots, fig.cap="Distribution of the Predictor Variables"}
winequality_noq = winequality[,-1]
data_long <- melt(winequality_noq)
ggplot(data_long, aes(x = qualityclass, y = value)) +            
  geom_boxplot() +
  facet_wrap(~variable, scales = "free")
```

```{r correlation, fig.cap= "Correlation Plot"}
corr <- round(cor(winequality[c(1, 4:14)]), 3)
p.mat <- cor_pmat(winequality[c(1, 4:14)])
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           outline.col = "white",
           colors = c("#6D9EC1", "white", "#E46726"),
           p.mat = p.mat,
           lab = TRUE,
           insig = "blank")

```

It was found that the majority class had 6053 observations whereas the minority class had 444 observations, a ratio of about 13:1. Due to this imbalanced nature of these data, classifying algorithms were biased towards classifying wines as Normal, and causing poor predictions for the Low and High classes. 

```{r accuracy-results, fig.cap = "MCMC accuracy results for each classifier applied to each resampling technique."}

# read in data
MCMC.results <- read_csv("MCMC.results.csv")

# new facet labels
accuracyGroup.labs <- c("Overall", "Low Quality", "Normal Quality", "High Quality")
names(accuracyGroup.labs) <- c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high")

# plot accuracy
MCMC.results %>%
  mutate(samplingMethod = factor(samplingMethod, levels = c("none", "undersample", "oversample")),
         accuracyGroup = factor(accuracyGroup, levels = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"))) %>%
  ggplot(aes(x = mean, y = samplingMethod, group = Method, color = Method)) +
  geom_point(size = 1, position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(xmin = lower, xmax = upper), position = position_dodge(width = 0.5), width = 0.4) +
  facet_wrap(~accuracyGroup, ncol = 1, labeller = labeller(accuracyGroup = accuracyGroup.labs)) +
  theme_bw() +
  theme(aspect.ratio = 0.15,
        legend.position = "bottom") +
  scale_y_discrete("") +
  scale_x_continuous("Accuracy", limits = c(0,1), breaks = seq(0,1,0.2), labels = scales::percent) +
  scale_color_brewer("", palette = "Paired")
```

```{r Random-Forest}}


# IMPORT DATA, RELEVEL FACTOR COLUMNS, 
winequality <- read_csv("data/winequality-all.csv") %>%
  mutate(type = factor(type, levels = c("red", "white")),
         type01 = as.numeric(ifelse(type == "white", 0, 1)),
         qualityclass = factor(qualityclass, levels = c("Low", "Normal", "High")),
         under_class = ifelse(qualityclass == "Normal", 0, 1),
         over_class = ifelse(qualityclass == "Low", 0, 
                             ifelse(qualityclass == "Normal", 1, 2)))
colnames(winequality) <- make.names(names(winequality), unique=TRUE)
summary(winequality)

# Drop quality variable
var.out <- !names(winequality) %in% c("quality")
winequality <- winequality[,var.out]

# FUNCTION FOR UNDERSAMPLING
undersample <- function(train_df, nsample){
  df_wine_0_ind <- which(train_df$under_class == 0)
  df_wine_1_ind <- which(train_df$under_class == 1)
  pick_0 <- sample(df_wine_0_ind, nsample)
  undersample_wine <- train_df[c(df_wine_1_ind,pick_0),] #Final Data frame
  undersample_wine <- undersample_wine %>%
    dplyr::select(qualityclass, type, fixed.acidity, volatile.acidity, citric.acid,
                  residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide,
                  density, pH, sulphates, alcohol)
  # table(undersample_wine$under_class) # have just to make sure it's all balancing out how I think
  return(undersample_wine)
}

# FUNCTION FOR OVERSAMPLING
oversample <- function(train_df, k){
  
  winequality_low <- filter(winequality, qualityclass %in% c('Low'))
  winequality_normal <- filter(winequality, qualityclass %in% c('Normal'))
  winequality_high <- filter(winequality, qualityclass %in% c('High'))
  
  wine <- sort(sample(nrow(winequality_normal), nrow(winequality_normal)*0.5))
  winequality_norm1 <- winequality_normal[wine,]
  winequality_norm2 <- winequality_normal[-wine,]
  
  wine_LN <- rbind(winequality_low,winequality_norm1)
  wine_HN <- rbind(winequality_high,winequality_norm2)
  SMOTEData1 <- SMOTE(wine_LN[,c("fixed.acidity", "volatile.acidity", "citric.acid",
                                 "residual.sugar", "chlorides", "free.sulfur.dioxide",
                                 "total.sulfur.dioxide", "density", "pH", "sulphates", "alcohol",
                                 "type01")], wine_LN[,"over_class"], K = k, dup_size = 0)
  SMOTEData2 <- SMOTE(wine_HN[,c("fixed.acidity", "volatile.acidity", "citric.acid",
                                 "residual.sugar", "chlorides", "free.sulfur.dioxide",
                                 "total.sulfur.dioxide", "density", "pH", "sulphates", "alcohol",
                                 "type01")], wine_HN[,"over_class"], K = k, dup_size = 0)
  oversample_df1 <- SMOTEData1$data #Final data frame
  oversample_df2 <- SMOTEData2$data #Final data frame
  oversample_df <- rbind(oversample_df1,oversample_df2) %>%
    mutate(type01 = round(type01)) %>%
    mutate(type = as.factor(ifelse(type01 == 0, "white","red")),
           qualityclass = ifelse(class == 0, "Low", 
                                 ifelse(class == "1", "Normal","High"))) %>%
    mutate(qualityclass = factor(qualityclass, levels = c("Low", "Normal", "High"))) %>%
    dplyr::select(qualityclass, type, fixed.acidity, volatile.acidity, citric.acid,
                  residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide,
                  density, pH, sulphates, alcohol)
  #table(oversample_df$class) have just to make sure it's all balancing out how I think 
  return(oversample_df)
}

# SET UP FUNCTIONS TO EVALUATE RANDOM FOREST

# Fit random forest
rfFunc <- 
  function(df = winequality, samplingMethod = "none", nUndersample = 2000, kOversample = 5,
           trainPct = 0.7, importance = F, mtry = 4, ntree = 500, show.table = F){
    
    require(randomForest)
  
    # set up training/testing sets
    n <- nrow(df)
    train.index <- sample(seq(1,n), floor(n*trainPct), replace = F)
    
    # training
    train.data <-  df[train.index,] # Normal
    
    if(samplingMethod == "undersample"){
      train.data <- undersample(train.data, nUndersample) # Undersample
    }
    
    if(samplingMethod == "oversample"){
      train.data <- oversample(train.data, kOversample) # Oversample
    }
    
    train.data <- train.data %>%  
      dplyr::select(qualityclass, type, fixed.acidity, volatile.acidity, citric.acid,
                    residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide,
                    density, pH, sulphates, alcohol)

    # testing
    test.data  <- df[-train.index,] %>%     
      dplyr::select(qualityclass, type, fixed.acidity, volatile.acidity, citric.acid,
                    residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide,
                    density, pH, sulphates, alcohol)

    # Fit that Random Forest!
    rf.fit <- randomForest(qualityclass ~ ., 
                           data=train.data,
                           method="class", 
                           ntree=ntree,
                           mtry = mtry,
                           importance=importance
                           )

    # Get that Prediction! 
    rf.pred = predict(rf.fit, newdata = test.data) 

    # Evaluate Prediction
    accuracy.all  <- mean(rf.pred==test.data$qualityclass) #this is not working SJA
    table    <- table(test.data$qualityclass, rf.pred)
    prop.table <- table/rowSums(table)
    accuracy.low    <- prop.table[1,1]
    accuracy.normal <- prop.table[2,2]
    accuracy.high   <- prop.table[3,3]
    accuracy <- cbind(accuracy.all, accuracy.low, accuracy.normal, accuracy.high)
    
    if(show.table){
      return(list(accuracy = accuracy, table = table, prop.table = prop.table)) 
    } else{
      return(accuracy) 
    }
    
  }

# Select number of variables sampled at each split
rftuneFunc <- 
  function(df = winequality, samplingMethod = "none", nUndersample = 2000, kOversample = 5,
           trainPct = 0.7, 
           stepFactor = 1, doBest=TRUE){
    
    require(tuneRF)
    
    # set up training/testing sets
    n <- nrow(df)
    train.index <- sample(seq(1,n), floor(n*trainPct), replace = F)
    
    # training
    train.data <-  df[train.index,] # Normal
    
    if(samplingMethod == "undersample"){
      train.data <- undersample(train.data, nUndersample) # Undersample
    }
    
    if(samplingMethod == "oversample"){
      train.data <- oversample(train.data, kOversample) # Oversample
    }
    
    train.data <- train.data %>%  
      dplyr::select(qualityclass, type, fixed.acidity, volatile.acidity, citric.acid,
                    residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide,
                    density, pH, sulphates, alcohol)
    
    # testing
    test.data  <- df[-train.index,] %>%     
      dplyr::select(qualityclass, type, fixed.acidity, volatile.acidity, citric.acid,
                    residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide,
                    density, pH, sulphates, alcohol)
    
    # Tune that Random Forest!
    rf.tune <- tuneRF(x = train.data[,-1], y = train.data$qualityclass, stepFactor=stepFactor)

  }

# EVALUATE RANDOM FOREST AND MTRY

# Random Forest Accuracy

rf.none.results <- rfFunc(df = winequality, samplingMethod = "none", nUndersample = NA, kOversample = NA, trainPct = 0.7, ntree = 500, mtry = 4, show.table = T)
rf.none.results 

rf.undersample.results <- rfFunc(df = winequality, samplingMethod = "undersample", nUndersample = 2000, kOversample = NA, trainPct = 0.7, ntree = 500, mtry = 4, show.table = T)
rf.undersample.results

rf.oversample.results <- rfFunc(df = winequality, samplingMethod = "oversample", nUndersample = NA, kOversample = 5, trainPct = 0.7, ntree = 500, mtry = 4, show.table = T)
rf.oversample.results

rf.results <- rbind(rf.none.results$accuracy,
                     rf.undersample.results$accuracy,
                     rf.oversample.results$accuracy)
rownames(rf.results) <- c("None", "Undersampling", "Oversampling")

rf.results %>%
  as.data.frame() %>%
  rownames_to_column("SampleMethod") %>%
  pivot_longer(cols = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"),
               names_to = "AccuracyGroup",
               values_to = "Accuracy") %>%
  mutate(SampleMethod = factor(SampleMethod, levels = c("Oversampling", "Undersampling", "None")),
         AccruacyGroup = factor(AccuracyGroup, levels = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"))) %>%
ggplot(aes(x = Accuracy, y = SampleMethod)) +
  geom_point() +
  facet_wrap(~ AccuracyGroup, ncol = 1) +
  theme_bw() +
  theme(aspect.ratio = 0.2) +
  scale_y_discrete("") +
  scale_x_continuous("Classification Rate", limits = c(0,1), breaks = seq(0,1,0.2), labels = scales::percent)

# Random Forest Accuracy with Optimum Variable Selection

rftune.none.results <- rfFunc(df = winequality, samplingMethod = "none", nUndersample = NA, kOversample = NA, trainPct = 0.7)
rftune.none.results 

rftune.undersample.results <- rfFunc(df = winequality, samplingMethod = "undersample", nUndersample = 2000, kOversample = NA, trainPct = 0.7)
rftune.undersample.results

rftune.oversample.results <- rfFunc(df = winequality, samplingMethod = "oversample", nUndersample = NA, kOversample = 5, trainPct = 0.7)
rftune.oversample.results

rftune.results <- rbind(rftune.none.results,
                    rftune.undersample.results,
                    rftune.oversample.results)
rownames(rftune.results) <- c("None", "Undersampling", "Oversampling")

rftune.results %>%
  as.data.frame() %>%
  rownames_to_column("SampleMethod") %>%
  pivot_longer(cols = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"),
               names_to = "AccuracyGroup",
               values_to = "Accuracy") %>%
  mutate(SampleMethod = factor(SampleMethod, levels = c("Oversampling", "Undersampling", "None")),
         AccruacyGroup = factor(AccuracyGroup, levels = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"))) %>%
  ggplot(aes(x = Accuracy, y = SampleMethod)) +
  geom_point() +
  facet_wrap(~ AccuracyGroup, ncol = 1) +
  theme_bw() +
  theme(aspect.ratio = 0.2) +
  scale_y_discrete("") +
  scale_x_continuous("Classification Rate", limits = c(0,1), breaks = seq(0,1,0.2), labels = scales::percent)

# MCMC FUNCTION WITH PARALLEL COMPUTING
rfMCMC <- 
  function(B = 5, samplingMethod = "none", nUndersample = 2000, kOversample = 5, trainPct = 0.7, ntree = 500, mtry = 4){
    require(furrr)
    
    # Create Parameter Grid
    mcmc.grid <- expand_grid(B = seq(1,B),
                             samplingMethod = samplingMethod,
                             nUndersample = nUndersample,
                             kOversample = kOversample,
                             trainPct = trainPct,
                             ntree = ntree,
                             mtry = mtry)
    
    # Obtain Accuracy
    accuracyList <- furrr::future_pmap(mcmc.grid[,-1], rfFunc)
    rfAccuracy <- matrix(unlist(accuracyList, use.names = TRUE), ncol = 4, nrow = nrow(mcmc.grid), byrow = T)
    colnames(rfAccuracy) <- colnames(accuracyList[[1]])
    
    # Summarize Accuracy
    results <- cbind(mcmc.grid, rfAccuracy) %>%
      pivot_longer(cols = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"),
                   names_to = "accuracyGroup",
                   values_to = "accuracy") %>%
      mutate(Method = "Random Forest") %>%
      dplyr::group_by(Method, accuracyGroup, samplingMethod, nUndersample, kOversample, ntree, mtry) %>%
      summarise(B = n(),
                mean  = mean(accuracy),
                lower = quantile(accuracy, probs = c(0.05)),
                upper = quantile(accuracy, probs = c(0.95))) %>%
      ungroup()
    
    return(results)
    
  }

# HYPERPARAMETER GRID SEARCH
tic()
rfMCMC.none.results <- rfMCMC(B = 50, samplingMethod = "none", nUndersample = NA, kOversample = NA,
                                trainPct = 0.7, ntree = 500, mtry = 4)
toc()
# rfMCMC.none.results

tic()
rfMCMC.undersample.results <- rfMCMC(B = 50, samplingMethod = "undersample", nUndersample = 2000, kOversample = NA,
                                      trainPct = 0.7, ntree = 500, mtry = 4)
toc()
# rfMCMC.undersample.results

tic()
rfMCMC.oversample.results <- rfMCMC(B = 50, samplingMethod = "oversample", nUndersample = NA, kOversample = 5,
                                    trainPct = 0.7, ntree = 500, mtry = 4)
toc()
# rfMCMC.oversample.results


rfMCMC.results <- rbind(rfMCMC.none.results,
                        rfMCMC.undersample.results,
                        rfMCMC.oversample.results)

rfMCMC.results %>%
  mutate(samplingMethod = factor(samplingMethod, levels = c("none", "undersample", "oversample")),
         accuracyGroup = factor(accuracyGroup, levels = c("accuracy.all", "accuracy.low", "accuracy.normal", "accuracy.high"))) %>%
  ggplot(aes(x = mean, y = samplingMethod)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  facet_wrap(~accuracyGroup, ncol = 1) +
  theme_bw() +
  theme(aspect.ratio = 0.2) +
  scale_y_discrete("") +
  scale_x_continuous("Accuracy", limits = c(0,1), breaks = seq(0,1,0.2), labels = scales::percent) +
  ggtitle("Random Forest \nntree = 500; mtry = 4")

rfMCMC.results

```

###  Discussion  ###

Place references concerning performance of each technique here and contrast with our results

# Conclusion, Lessons Learned, and Future Directions

```{r, include = F}
# devtools::install_github("crsh/papaja") Alison - I'm commenting this all out for now
#library(papaja)
#render_appendix(
  #"appendix.Rmd",
  #bibliography = rmarkdown::metadata$bibliography,
  #csl = rmarkdown::metadata$csl,
  #quiet = TRUE
#)
```


